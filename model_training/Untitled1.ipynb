{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import warnings\n",
    "import scipy\n",
    "from scipy.stats import kurtosis, skew\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def numeric_impute(data, num_cols, method):\n",
    "    \n",
    "    num_data = data[num_cols]\n",
    "    if method == 'mode':\n",
    "        output = num_data.fillna(getattr(num_data, method)().iloc[0])\n",
    "    else:\n",
    "        output = num_data.fillna(getattr(num_data, method)())\n",
    "    return output\n",
    "\n",
    "def dict_merge(*args):\n",
    "    imp = {}\n",
    "    for dictt in args:\n",
    "        imp.update(dictt)\n",
    "    return imp\n",
    "\n",
    "\n",
    "def summary_stats(data, include_quantiles = False):\n",
    "    quantiles = np.quantile(data,[0, 0.25, 0.75, 1])\n",
    "    minn = quantiles[0]\n",
    "    maxx = quantiles[-1]\n",
    "    q1 = quantiles[1]\n",
    "    q3 = quantiles[2]\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    \n",
    "    if include_quantiles:\n",
    "        return minn, q1, mean, std, q3, maxx\n",
    "    else:\n",
    "        return minn, mean, std, maxx\n",
    "\n",
    "def pair_corr(data):    \n",
    "    cors = abs(data.corr().values)\n",
    "    cors = np.triu(cors,1).flatten()\n",
    "    cors = cors[cors != 0]\n",
    "    return cors\n",
    "\n",
    "def calc_MI(x, y, bins):\n",
    "    c_xy = np.histogram2d(x, y, bins)[0]\n",
    "    mi = mutual_info_score(None, None, contingency=c_xy)\n",
    "    return mi\n",
    "\n",
    "def MI(X, y):\n",
    "    bins = 10\n",
    "    \n",
    "    # check if X and y have the same length\n",
    "    n = X.shape[1]\n",
    "    matMI = np.zeros(n)\n",
    "\n",
    "    for ix in np.arange(n):\n",
    "        matMI[ix] = calc_MI(X.iloc[:,ix], y, bins)\n",
    "    \n",
    "    return matMI\n",
    "\n",
    "def preprocessing(data):\n",
    "    \n",
    "    X = data.iloc[:, :-1]\n",
    "\n",
    "    # selecting the response variable\n",
    "    y = data.iloc[:, -1]\n",
    "\n",
    "    # one-hot encoding\n",
    "    X = pd.get_dummies(X)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "def meta_features(data, num_cols, categorical_cols):\n",
    "    \n",
    "    metafeatures = {}\n",
    "    \n",
    "    target_variable = data.iloc[:, -1]\n",
    "    \n",
    "    nr_classes = target_variable.nunique()   \n",
    "    metafeatures['nr_classes'] = nr_classes\n",
    "\n",
    "    \n",
    "    nr_instances = data.shape[0]\n",
    "    metafeatures['nr_instances'] = nr_instances\n",
    "    \n",
    "    log_nr_instances = np.log(nr_instances)\n",
    "    metafeatures['log_nr_instances'] = log_nr_instances\n",
    "    \n",
    "    nr_features = data.shape[1]\n",
    "    metafeatures['nr_features'] = nr_features\n",
    "    \n",
    "    log_nr_features = np.log(nr_features)\n",
    "    metafeatures['log_nr_features'] = log_nr_features\n",
    "    \n",
    "    missing_val = data.isnull().sum().sum() + data.isna().sum().sum()\n",
    "    metafeatures['missing_val'] = missing_val\n",
    "    \n",
    "    # Ratio of Missing Values \n",
    "    ratio_missing_val = missing_val / data.size\n",
    "    #  metafeatures['ratio_missing_val'] = ratio_missing_val\n",
    "    \n",
    "    # Number of Numerical Features \n",
    "    nr_numerical_features = len(num_cols)\n",
    "    #  metafeatures['nr_numerical_features'] = nr_numerical_features\n",
    "    \n",
    "    # Number of Categorical Features \n",
    "    nr_categorical_features = len(categorical_cols)\n",
    "    metafeatures['nr_categorical_features'] = nr_categorical_features\n",
    "    \n",
    "    # print(data[num_cols].nunique() / data[num_cols].count()) \n",
    "    \n",
    "    \n",
    "    # Ratio of Categorical to Numerical Features \n",
    "    if nr_numerical_features != 0:\n",
    "        ratio_num_cat = nr_categorical_features / nr_numerical_features\n",
    "    else:\n",
    "        ratio_num_cat = 'NaN'\n",
    "    \n",
    "    #  metafeatures['ratio_num_cat'] = ratio_num_cat\n",
    "        \n",
    "    # Dataset Ratio\n",
    "    dataset_ratio = nr_features / nr_instances\n",
    "    metafeatures['dataset_ratio'] = dataset_ratio\n",
    "        \n",
    "    # Categorical Features Statistics\n",
    "    if nr_categorical_features != 0:\n",
    "\n",
    "        labels = data[categorical_cols].nunique()\n",
    "\n",
    "        # Labels Sum \n",
    "        labels_sum = np.sum(labels)\n",
    "        \n",
    "        # Labels Mean \n",
    "        labels_mean = np.mean(labels)\n",
    "\n",
    "        # Labels Std \n",
    "        labels_std = np.std(labels)\n",
    "        \n",
    "    else:\n",
    "        labels_sum = 0\n",
    "        labels_mean = 0\n",
    "        labels_std = 0\n",
    "        \n",
    "    #  metafeatures['labels_sum'] = labels_sum\n",
    "    metafeatures['labels_mean'] = labels_mean\n",
    "    metafeatures['labels_std'] = labels_std\n",
    "\n",
    "    return metafeatures\n",
    "\n",
    "\n",
    "\n",
    "def meta_features2(data, num_cols):\n",
    "    \n",
    "    metafeatures = {}\n",
    "     \n",
    "    nr_numerical_features = len(num_cols)\n",
    "    \n",
    "    if nr_numerical_features != 0:\n",
    "        \n",
    "        skewness_values = abs(data[num_cols].skew())\n",
    "        kurtosis_values = data[num_cols].kurtosis()        \n",
    "                \n",
    "        skew_min, skew_q1, \\\n",
    "        skew_mean, skew_std, \\\n",
    "        skew_q3, skew_max = summary_stats(skewness_values, \n",
    "                                          include_quantiles=True)\n",
    "        \n",
    "        kurtosis_min, kurtosis_q1, \\\n",
    "        kurtosis_mean, kurtosis_std, \\\n",
    "        kurtosis_q3, kurtosis_max = summary_stats(kurtosis_values,\n",
    "                                                  include_quantiles=True)\n",
    "        \n",
    "               \n",
    "        pairwise_correlations = pair_corr(data[num_cols])\n",
    "                \n",
    "        try:\n",
    "            rho_min, rho_mean, \\\n",
    "            rho_std, rho_max = summary_stats(pairwise_correlations)\n",
    "        except IndexError:\n",
    "            pass\n",
    "                    \n",
    "    var_names = ['skew_min', \n",
    "                'skew_std', 'skew_mean',\n",
    "                'skew_q1', 'skew_q3', 'skew_max',\n",
    "                'kurtosis_min', 'kurtosis_std',\n",
    "                'kurtosis_mean', 'kurtosis_q1',\n",
    "                'kurtosis_q3', 'kurtosis_max',\n",
    "                'rho_min', 'rho_max', 'rho_mean',\n",
    "                'rho_std']\n",
    "\n",
    "    for var in var_names:\n",
    "        try:\n",
    "            metafeatures[var] = eval(var)            \n",
    "        except NameError:           \n",
    "            metafeatures[var] = 0\n",
    "            \n",
    "\n",
    "    return metafeatures\n",
    "\n",
    "def shan_entropy(c):\n",
    "    c_normalized = c[np.nonzero(c)[0]]\n",
    "    H = -sum(c_normalized* np.log2(c_normalized))  \n",
    "    return H\n",
    "\n",
    "def norm_entropy(X):\n",
    "    bins = 10\n",
    "    nr_features = X.shape[1]\n",
    "    n = X.shape[0]\n",
    "    H = np.zeros(nr_features)\n",
    "    for i in range(nr_features):\n",
    "        x = X.iloc[:,i]\n",
    "        cont = len(np.unique(x)) > bins\n",
    "        if cont:\n",
    "            # discretizing cont features \n",
    "            x_discr = np.histogram(x, bins)[0]\n",
    "            x_norm = x_discr / float(np.sum(x_discr))\n",
    "            H_x = shan_entropy(x_norm)\n",
    "            \n",
    "        else:\n",
    "            x_norm = x.value_counts().values / n \n",
    "            H_x = shan_entropy(x_norm)            \n",
    "        H[i] = H_x\n",
    "    H /= np.log2(n)\n",
    "    return H\n",
    "\n",
    "def meta_features_info_theoretic(X, y):\n",
    "    \n",
    "    metafeatures = {}\n",
    "    nr_instances = X.shape[0]\n",
    "    \n",
    "    # Class Entropy\n",
    "    class_probs = np.bincount(y) / nr_instances \n",
    "    class_entropy = shan_entropy(class_probs)\n",
    "    metafeatures['class_entropy'] = class_entropy\n",
    "\n",
    "    # Class probability    \n",
    "    metafeatures['prob_min'], \\\n",
    "    metafeatures['prob_mean'], \\\n",
    "    metafeatures['prob_std'], \\\n",
    "    metafeatures['prob_max'] = summary_stats(class_probs)\n",
    "\n",
    "    # Norm. attribute entropy\n",
    "    H = norm_entropy(X) \n",
    "    metafeatures['norm_entropy_min'], \\\n",
    "    metafeatures['norm_entropy_mean'], \\\n",
    "    metafeatures['norm_entropy_std'], \\\n",
    "    metafeatures['norm_entropy_max'] = summary_stats(H)\n",
    "    \n",
    "    # Mutual information\n",
    "    mutual_information = MI(X, y)\n",
    "    metafeatures['mi_min'], \\\n",
    "    metafeatures['mi_mean'], \\\n",
    "    metafeatures['mi_std'], \\\n",
    "    metafeatures['mi_max'] = summary_stats(mutual_information)\n",
    "    \n",
    "    # Equiv. nr. of features\n",
    "    metafeatures['equiv_nr_feat'] = metafeatures['class_entropy'] / metafeatures['mi_mean']\n",
    "    \n",
    "    # Noise-signal ratio\n",
    "    noise = metafeatures['norm_entropy_mean'] - metafeatures['mi_mean']\n",
    "    metafeatures['noise_signal_ratio'] = noise / metafeatures['mi_mean']\n",
    "    \n",
    "    return metafeatures\n",
    "\n",
    "class LandmarkerModel:\n",
    "    \n",
    "    def __init__(self, model, X_train, y_train, X_test, y_test):\n",
    "        self.model = model\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def accuracy(self):\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        predictions = self.model.predict(self.X_test)\n",
    "        CV_accuracy = accuracy_score(self.y_test, predictions)          \n",
    "        return CV_accuracy\n",
    "    \n",
    "def meta_features_landmarkers(X, y):\n",
    "    \n",
    "    metafeatures = {}\n",
    "    \n",
    "    k = 10\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "    \n",
    "    model_1nn = KNeighborsClassifier(n_neighbors=1)\n",
    "    model_dt = DecisionTreeClassifier()\n",
    "    model_gnb = GaussianNB()\n",
    "    model_lda = LinearDiscriminantAnalysis()\n",
    "    \n",
    "    \n",
    "    CV_accuracy_1nn = 0\n",
    "    CV_accuracy_dt = 0\n",
    "    CV_accuracy_gnb = 0\n",
    "    CV_accuracy_lda = 0\n",
    "    \n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "            \n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "         \n",
    "        CV_accuracy_1nn += LandmarkerModel(model_1nn, X_train, y_train, X_test, y_test).accuracy()\n",
    "        CV_accuracy_dt += LandmarkerModel(model_dt, X_train, y_train, X_test, y_test).accuracy()\n",
    "        CV_accuracy_gnb += LandmarkerModel(model_gnb, X_train, y_train, X_test, y_test).accuracy()\n",
    "        \n",
    "        try:\n",
    "            CV_accuracy_lda += LandmarkerModel(model_lda, X_train, y_train, X_test, y_test).accuracy()             \n",
    "        except scipy.linalg.LinAlgError:\n",
    "            pass    \n",
    "    \n",
    "    CV_accuracy_1nn /= k\n",
    "    CV_accuracy_dt /= k\n",
    "    CV_accuracy_gnb /= k\n",
    "    CV_accuracy_lda /= k\n",
    "    \n",
    "    metafeatures['Landmarker_1NN'] = CV_accuracy_1nn\n",
    "    metafeatures['Landmarker_dt'] = CV_accuracy_dt\n",
    "    metafeatures['Landmarker_gnb'] = CV_accuracy_gnb\n",
    "    metafeatures['Landmarker_lda'] = CV_accuracy_lda\n",
    "        \n",
    "    return metafeatures\n",
    "\n",
    "def all_metafeatures(data, num_cols, metafeatures1):\n",
    "\n",
    "    metafeatures2 = meta_features2(data, num_cols)\n",
    "    X, y = preprocessing(data)                             \n",
    "    metafeatures3 = meta_features_info_theoretic(X, y)\n",
    "    #metafeatures4 = meta_features_landmarkers(X, y)  \n",
    "\n",
    "    #metafeatures = dict_merge(metafeatures1, metafeatures2,\n",
    "    #                          metafeatures3, metafeatures4)\n",
    "    metafeatures = dict_merge(metafeatures1, metafeatures2,\n",
    "                              metafeatures3)\n",
    "    return metafeatures\n",
    "\n",
    "def extract_metafeatures(file):\n",
    "  \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    data = pd.read_csv(file,\n",
    "                       index_col=None,\n",
    "                       header=0,\n",
    "                       na_values='?')\n",
    "\n",
    "    data.columns = map(str.lower, data.columns)\n",
    "\n",
    "    # removing an id column if exists\n",
    "    if 'id' in data.columns:\n",
    "        data = data.drop('id', 1)\n",
    "        \n",
    "    # remove constant columns\n",
    "    data = data.loc[:, (data != data.iloc[0]).any()]\n",
    "    const_col = data.std().index[data.std() == 0]\n",
    "    data = data.drop(const_col,axis=1) \n",
    "    \n",
    "    # remove columns with only NaN values\n",
    "    empty_cols = ~data.isna().all()\n",
    "    data = data.loc[:, empty_cols]\n",
    "    \n",
    "    cols = set(data.columns)\n",
    "    num_cols = set(data._get_numeric_data().columns)\n",
    "    categorical_cols = list(cols.difference(num_cols))\n",
    "\n",
    "    # data imputation for categorical features\n",
    "    categ_data = data[categorical_cols]\n",
    "    \n",
    "    data[categorical_cols] = categ_data.fillna(categ_data.mode().iloc[0])\n",
    "    \n",
    "       \n",
    "    metafeatures1 = meta_features(data, num_cols, categorical_cols)\n",
    "    \n",
    "    ### Numerical Features Statistics  \n",
    "    missing_val = metafeatures1['missing_val'] \n",
    "        \n",
    "    if missing_val != 0:\n",
    "        \n",
    "        imputation_types = ['mean', 'median', 'mode']\n",
    "\n",
    "        imputed_data = data.copy()\n",
    "    \n",
    "        results = pd.DataFrame()\n",
    "        for index, num_imput_type in enumerate(imputation_types):\n",
    "                       \n",
    "            num_cols = list(num_cols) \n",
    "            imputed_data[num_cols] = numeric_impute(data, num_cols, num_imput_type)\n",
    "            #metafeatures1['num_imput_type'] = num_imput_type\n",
    "            metafeatures = all_metafeatures(imputed_data, num_cols, metafeatures1)                     \n",
    "            \n",
    "            df = pd.DataFrame([metafeatures])\n",
    "            results = pd.concat([results, df], axis=0)\n",
    "    else:\n",
    "        \n",
    "        #metafeatures1['num_imput_type'] = None\n",
    "        metafeatures = all_metafeatures(data, num_cols, metafeatures1)\n",
    "        \n",
    "        results = pd.DataFrame([metafeatures])\n",
    "    \n",
    "    dataset_name = file.split('\\\\')[-1]\n",
    "    \n",
    "    results['dataset'] = dataset_name\n",
    "    \n",
    "    return  results\n",
    "\n",
    "def extract_for_all(path):\n",
    " \n",
    "    allFiles = glob.glob(path + \"*.csv\")\n",
    "\n",
    "    results = pd.DataFrame()\n",
    "    for idx, file in enumerate(allFiles):\n",
    "        d_name = file.split('//')[-1]\n",
    "        print('Dataset {}({})'.format(idx + 1, d_name))\n",
    "        results = pd.concat([results, extract_metafeatures(file)], axis=0)\n",
    "\n",
    "    results.to_csv('metafeatures.csv',\n",
    "                   header=True,\n",
    "                   index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a=extract_metafeatures(\"blood.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nr_classes', 'nr_instances', 'log_nr_instances', 'nr_features',\n",
       "       'log_nr_features', 'missing_val', 'nr_categorical_features',\n",
       "       'dataset_ratio', 'labels_mean', 'labels_std', 'skew_min', 'skew_std',\n",
       "       'skew_mean', 'skew_q1', 'skew_q3', 'skew_max', 'kurtosis_min',\n",
       "       'kurtosis_std', 'kurtosis_mean', 'kurtosis_q1', 'kurtosis_q3',\n",
       "       'kurtosis_max', 'rho_min', 'rho_max', 'rho_mean', 'rho_std',\n",
       "       'class_entropy', 'prob_min', 'prob_mean', 'prob_std', 'prob_max',\n",
       "       'norm_entropy_min', 'norm_entropy_mean', 'norm_entropy_std',\n",
       "       'norm_entropy_max', 'mi_min', 'mi_mean', 'mi_std', 'mi_max',\n",
       "       'equiv_nr_feat', 'noise_signal_ratio', 'dataset'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nr_instances                      748\n",
       "log_nr_instances               6.6174\n",
       "nr_features                         6\n",
       "log_nr_features               1.79176\n",
       "missing_val                         0\n",
       "nr_categorical_features             0\n",
       "dataset_ratio              0.00802139\n",
       "labels_mean                         0\n",
       "labels_std                          0\n",
       "num_imput_type                   None\n",
       "skew_min                            0\n",
       "skew_std                      1.19796\n",
       "skew_mean                     1.71426\n",
       "skew_q1                      0.870372\n",
       "skew_q3                       2.87856\n",
       "skew_max                      3.21127\n",
       "kurtosis_min                     -1.2\n",
       "kurtosis_std                  7.50222\n",
       "kurtosis_mean                 6.53623\n",
       "kurtosis_q1                 -0.421909\n",
       "kurtosis_q3                    14.255\n",
       "kurtosis_max                  15.8762\n",
       "rho_min                     0.0358544\n",
       "rho_max                             1\n",
       "rho_mean                     0.310364\n",
       "rho_std                      0.251341\n",
       "class_entropy                0.791645\n",
       "prob_min                     0.237968\n",
       "prob_mean                         0.5\n",
       "prob_std                     0.262032\n",
       "prob_max                     0.762032\n",
       "norm_entropy_min             0.158788\n",
       "norm_entropy_mean            0.234211\n",
       "norm_entropy_std            0.0827204\n",
       "norm_entropy_max             0.347957\n",
       "mi_min                     0.00679654\n",
       "mi_mean                     0.0350004\n",
       "mi_std                      0.0200916\n",
       "mi_max                      0.0616487\n",
       "equiv_nr_feat                 22.6181\n",
       "noise_signal_ratio            5.69167\n",
       "Landmarker_1NN               0.695387\n",
       "Landmarker_dt                0.707297\n",
       "Landmarker_gnb               0.749964\n",
       "Landmarker_lda               0.772757\n",
       "dataset                     blood.csv\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
